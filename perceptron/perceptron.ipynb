{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed75cf4",
   "metadata": {},
   "source": [
    "# The Humble Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077fb437",
   "metadata": {},
   "source": [
    "## History\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce5dd8",
   "metadata": {},
   "source": [
    "In 1957, Frank Rosenblatt pioneered the first machine learning algorithm through the invention of the perceptron, modelled after biological neurons. Later unveiled to the public in 1958, the perceptron was touted as the next big thing with many believing that it would soon be able to \"walk, talk, see, write, reproduce itself and be conscious of its existence\"<sup>[1]</sup> (with some of the former only coming to fruition some 60 years later). The Navy claimed it was the first machine capable of \"receiving, recognizing and identifying its surroundings witouth any human training or control\"<sup>[1]</sup>, despite the overhyped expectations, the perceptron paved the way for the field of AI. However, rather unfortunately its shortcomings like its inability to learn patterns that were not linearly-sperable led AI research to stall for the coming decades, until the advent of the multilayer perceptron and backpropagation in the 1980s. \n",
    "\n",
    "<small>\n",
    "[1]: <a href=\"nytimes.com/timesmachine/1958/07/08/83417341.html?pageNumber=25\">“New Navy Device Learns By Doing,” New York Times, July 8 1958.</a>\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d90cb",
   "metadata": {},
   "source": [
    "## Math behind it\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51b402",
   "metadata": {},
   "source": [
    "**Perceptron Model**\n",
    "\n",
    "The perceptron is a binary classifier that maps an input vector $ \\mathbf{X} $ to a binary output $ \\hat{y} \\in \\{0, 1\\} $. It uses a simple linear model followed by a step activation function, defined as follows: \n",
    "\n",
    "$$\n",
    "z = \\mathbf{W} \\cdot \\mathbf{X} + b \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathrm{H}(z) = \\mathrm{H}( \\mathbf{W} \\cdot \\mathbf{X} + b  )\n",
    "\n",
    "= \n",
    "\n",
    "\\begin{cases}\n",
    "    1, & \\text{if  } \\mathbf{W} \\cdot \\mathbf{X} + b  \\ge 0, \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "- $ \\mathbf{W} = [w_1, w_2, \\dots, w_n] $: weights vector\n",
    "- $ \\mathbf{X} = [x_1, x_2, \\dots, x_n] $: input vector (features)\n",
    "- $ b $: bias term \n",
    "- $ \\hat{y} $: predicted label (0 or 1)\n",
    "- $ \\mathrm{H}() $: [step activation function](../activation_functions/step/step.ipynb)\n",
    "\n",
    "We can do a neat little simplification by absorbing the bias into weights by defining $ x_0 = 1 $, giving us: \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathrm{H}( \\mathbf{W} \\cdot \\mathbf{X}) = \\mathrm{H}( \\sum_{i=0}^n w_i x_i )\n",
    "$$\n",
    "\n",
    "Where: \n",
    "- $ x_0 = 1 $\n",
    "- $ w_0 = b $ \n",
    "\n",
    "<br><br>\n",
    "**Activation Function**\n",
    "\n",
    "The activation function is a Heaviside step function: \n",
    "\n",
    "$$\n",
    "\\mathrm{H}(z) = \\begin{cases}\n",
    "    1, & \\text{if  } z  \\ge 0, \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "> For a more in-depth intuition I have implemented the step function [here](../activation_functions/step/step.ipynb)\n",
    "\n",
    "\n",
    "<br><br>\n",
    "**Perceptron Update Rule**\n",
    "\n",
    "When training a perceptron we try to nudge the predicted label $ \\hat{y} $ towards the actual label $ y $, by updating the weights as follows: \n",
    "\n",
    "$$\n",
    "w_i \\leftarrow w_i +  \\eta \\cdot (y - \\hat{y}) \\cdot x_i \n",
    "$$\n",
    "\n",
    "Where: \n",
    "- $ \\eta $: the learning rate\n",
    "\n",
    "\n",
    "> note:  \n",
    "> The error $ (y - \\hat{y}) $ will resolve to one of 3 cases\n",
    "> - 0: the perceptron correctly classifies the input, so do nothing\n",
    "> - 1: the output is too low (0 when it should have been a 1), so increase weights in direction of input\n",
    "> - -1: the output is too high (1 when it should have been a 0), so decrease weights in direction of input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f1dca",
   "metadata": {},
   "source": [
    "## Code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159ce61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
